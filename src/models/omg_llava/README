# OMG_Llava
## Description
OMG_LLaVA is a fine-tuned multimodal vision-language model that extends the LLaVA (Large Language and Vision Assistant) architecture with object-level segmentation capabilities.

## Key Characteristics:
- Base Architecture: It's built on top of InternLM2-Chat-7B as the language model backbone
- Visual Encoder: Uses ConvNeXt-Large with OpenCLIP as the visual backbone
- Special Features:
    Object-level segmentation capabilities
    Visual prompting (region, point, box annotations)
    Panoptic segmentation decoder
    Special tokens for segmentation ([SEG], <region>, <mark>, etc.)

## Fine-tuning Strategy:
1. Pretraining Stage (omg_llava_7b_pretrain_8gpus.py)
2. Fine-tuning Stage (omg_llava_7b_finetune_8gpus.py)

## HuggingFace Project:
There is no README or tutorial in the project and the only available files are the following
1. internlm2-chat-7b: A folder contianing the InternLM2-Chat-7B model.
2. omg_llava_7b_pretrain_8gpus.pth: A Pytorch checkpoint of Lora wieght, which I am guessing are the wieghts of the model after the first pretraining stage.
This checkpoint has 17 layers with the following names:
  1. llm.model.tok_embeddings.weight          | Shape: torch.Size([92548, 4096])
  2. llm.output.weight                        | Shape: torch.Size([92548, 4096])
  3. projector.model.query_proj.weight        | Shape: torch.Size([6144, 512])
  4. projector.model.query_proj.bias          | Shape: torch.Size([6144])
  5. projector.model.model.0.weight           | Shape: torch.Size([4096, 6144])
  6. projector.model.model.0.bias             | Shape: torch.Size([4096])
  7. projector.model.model.2.weight           | Shape: torch.Size([4096, 4096])
  8. projector.model.model.2.bias             | Shape: torch.Size([4096])
  9. projector.model.model_feat.0.weight      | Shape: torch.Size([4096, 6656])
 10. projector.model.model_feat.0.bias        | Shape: torch.Size([4096])
 11. projector.model.model_feat.2.weight      | Shape: torch.Size([4096, 4096])
 12. projector.model.model_feat.2.bias        | Shape: torch.Size([4096])
 13. projector.model.seperate_embed.weight    | Shape: torch.Size([1, 4096])
 14. projector_text2vision.model.0.weight     | Shape: torch.Size([512, 4096])
 15. projector_text2vision.model.0.bias       | Shape: torch.Size([512])
 16. projector_text2vision.model.2.weight     | Shape: torch.Size([512, 512])
 17. projector_text2vision.model.2.bias       | Shape: torch.Size([512])

3. omg_llava_7b_finetune_8gpus.pth: A Pytorch checkpoint of Lora wieght, which I am guessing are the wieghts of the model after the first fine-tuning stage.
This checkpoint has 449 layers with the following 20 names:
  1) llm.base_model.model.model.tok_embeddings.weight | Shape: torch.Size([92548, 4096])
  2) llm.base_model.model.output.base_layer.weight | Shape: torch.Size([92548, 4096])
  3) llm.base_model.model.model.layers.0.attention.wqkv.lora_A.default.weight | Shape: torch.Size([512, 4096])
  4) llm.base_model.model.model.layers.0.attention.wqkv.lora_B.default.weight | Shape: torch.Size([6144, 512])
  5) llm.base_model.model.model.layers.0.attention.wo.lora_A.default.weight | Shape: torch.Size([512, 4096])
  6) llm.base_model.model.model.layers.0.attention.wo.lora_B.default.weight | Shape: torch.Size([4096, 512])
  7) llm.base_model.model.model.layers.0.feed_forward.w1.lora_A.default.weight | Shape: torch.Size([512, 4096])
  8) llm.base_model.model.model.layers.0.feed_forward.w1.lora_B.default.weight | Shape: torch.Size([14336, 512])
  9) llm.base_model.model.model.layers.0.feed_forward.w3.lora_A.default.weight | Shape: torch.Size([512, 4096])
 10) llm.base_model.model.model.layers.0.feed_forward.w3.lora_B.default.weight | Shape: torch.Size([14336, 512])
 11) llm.base_model.model.model.layers.0.feed_forward.w2.lora_A.default.weight | Shape: torch.Size([512, 14336])
 12) llm.base_model.model.model.layers.0.feed_forward.w2.lora_B.default.weight | Shape: torch.Size([4096, 512])
 13) llm.base_model.model.model.layers.1.attention.wqkv.lora_A.default.weight | Shape: torch.Size([512, 4096])
 14) llm.base_model.model.model.layers.1.attention.wqkv.lora_B.default.weight | Shape: torch.Size([6144, 512])
 15) llm.base_model.model.model.layers.1.attention.wo.lora_A.default.weight | Shape: torch.Size([512, 4096])
 16. llm.base_model.model.model.layers.1.attention.wo.lora_B.default.weight | Shape: torch.Size([4096, 512])
 17. llm.base_model.model.model.layers.1.feed_forward.w1.lora_A.default.weight | Shape: torch.Size([512, 4096])
 18. llm.base_model.model.model.layers.1.feed_forward.w1.lora_B.default.weight | Shape: torch.Size([14336, 512])
 19. llm.base_model.model.model.layers.1.feed_forward.w3.lora_A.default.weight | Shape: torch.Size([512, 4096])
 20. llm.base_model.model.model.layers.1.feed_forward.w3.lora_B.default.weight | Shape: torch.Size([14336, 512])
