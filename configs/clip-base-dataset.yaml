architecture: clip
model_path: openai/clip-vit-base-patch32
model:
  - torch_dtype: auto
dataset:
  - text_dataset_path: wonderwind271/CLEVR_val_categories
  - image_dataset_path: /h/hsheta/vlm-competence-dev/data/CLEVR-mini
  - text_split: color
  - image_split: val
  - text_column: Q
  - image_column: image_filename
output_db: clip.db
modules:
  - vision_model.encoder
