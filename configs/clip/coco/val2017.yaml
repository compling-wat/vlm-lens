architecture: clip
model_path: openai/clip-vit-base-patch32
model:
  - torch_dtype: auto
dataset:
  - text_dataset_path: compling/coco-val2017-obj-qa-categories
  - image_dataset_path: /path/to/your/COCO   # downloaded using configs/dataset/coco.yaml
  - text_split: val2017
  - image_split: val2017
  - prompt_column: question
  - label_column: answer
  - image_column: image_id
  - image_regex: "{id:0>12}.jpg"
output_db: clip-coco.db
pooled_output: True
modules:
  - text_model.encoder.layers.5.layer_norm1
  - text_model.encoder.layers.11.layer_norm1
