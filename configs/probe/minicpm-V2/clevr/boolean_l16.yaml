model:
  - activation: ReLU
  - hidden_size: 512
  - num_layers: 2
  - save_dir: minicpm-V2_boolean_probe_l19
training:
  - batch_size: [64, 128, 1024]
  - num_epochs: [50, 100, 200]
  - learning_rate: [0.001, 0.0005, 0.0001]
  - optimizer: AdamW
  - loss: CrossEntropyLoss
test:
  - batch_size: 32
  - loss: CrossEntropyLoss
data:
  - input_db: /projects/compling/hsheta/minicpm-V2/clevr/minicpm-V2-boolean.db
  - db_name: tensors
  - input_layer: llm.model.layers.19.post_attention_layernorm
