model:
  - activation: ReLU
  - hidden_size: 512
  - num_layers: 2
  - save_dir: glamm_boolean_probe_l15
training:
  - batch_size: [64, 128, 1024]
  - num_epochs: [50, 100, 200]
  - learning_rate: [0.001, 0.0005, 0.0001]
  - optimizer: AdamW
  - loss: CrossEntropyLoss
test:
  - batch_size: 32
  - loss: CrossEntropyLoss
data:
  - input_db: glamm/glamm-boolean.db
  - db_name: tensors
  - input_layer: language_model.model.layers.15.post_attention_layernorm
