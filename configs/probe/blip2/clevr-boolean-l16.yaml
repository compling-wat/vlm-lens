model:
  - activation: ReLU
  - hidden_size: 512
  - num_layers: 2
  - save_dir: blip2_boolean_probe_l16
training:
  - batch_size: [64, 128, 1024]
  - num_epochs: [50, 100, 200]
  - learning_rate: [0.001, 0.0005, 0.0001]
  - optimizer: AdamW
  - loss: CrossEntropyLoss
test:
  - batch_size: 32
  - loss: CrossEntropyLoss
data:
  - input_db: blip2/blip2-boolean.db
  - db_name: tensors
  - input_layer: language_model.model.decoder.layers.16.self_attn_layer_norm
